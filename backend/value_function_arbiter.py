from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional
import numpy as np
import os
import redis

app = FastAPI(title="Value Function Arbiter", description="Arbitrates between multiple model/engine outputs using trust, context, and ROI.")

# --- Trust/Confidence Scoring (stub: in-memory, can use Redis) ---
TRUST_SCORES = {
    "gnn_reasoning": 0.92,
    "materials_bert": 0.91,
    "ai_feedback_orchestrator": 0.87,
    "advanced_analytics": 0.93,
    "ai_pricing": 0.89,
    "logistics": 0.88
}

# TODO: Replace with Redis/Postgres for distributed trust/confidence scoring
# redis_client = redis.Redis(host=os.getenv('REDIS_HOST', 'localhost'), port=int(os.getenv('REDIS_PORT', 6379)), db=0, decode_responses=True)

class CandidateResponse(BaseModel):
    model_id: str
    response: Any
    confidence: float = Field(..., ge=0.0, le=1.0)
    trust: Optional[float] = None
    cost: Optional[float] = None
    context: Optional[Dict[str, Any]] = None
    reward: Optional[float] = None

class ArbitrationRequest(BaseModel):
    candidates: List[CandidateResponse]
    arbitration_policy: str = Field("hybrid", description="Policy: logit_weighted, reward_model, hybrid")
    context: Optional[Dict[str, Any]] = None

class ArbitrationResult(BaseModel):
    selected_model: str
    selected_response: Any
    rationale: str
    scores: Dict[str, float]

@app.post("/arbiter/arbitrate", response_model=ArbitrationResult)
def arbitrate(request: ArbitrationRequest):
    candidates = request.candidates
    policy = request.arbitration_policy
    # Fill trust/confidence from TRUST_SCORES if not provided
    for c in candidates:
        if c.trust is None:
            c.trust = TRUST_SCORES.get(c.model_id, 0.5)
    # --- Arbitration Policies ---
    scores = {}
    if policy == "logit_weighted":
        for c in candidates:
            scores[c.model_id] = float(c.confidence) * float(c.trust)
    elif policy == "reward_model":
        for c in candidates:
            scores[c.model_id] = float(c.reward) if c.reward is not None else 0.0
    else:  # hybrid
        for c in candidates:
            base = float(c.confidence) * float(c.trust)
            reward = float(c.reward) if c.reward is not None else 0.0
            scores[c.model_id] = 0.7 * base + 0.3 * reward
    # Select best
    selected_model = max(scores, key=scores.get)
    selected_response = next(c.response for c in candidates if c.model_id == selected_model)
    rationale = f"Selected {selected_model} with score {scores[selected_model]:.3f} using {policy} policy."
    return ArbitrationResult(
        selected_model=selected_model,
        selected_response=selected_response,
        rationale=rationale,
        scores=scores
    )

@app.get("/arbiter/trust-scores")
def get_trust_scores():
    # TODO: Replace with Redis/Postgres lookup
    return {"trust_scores": TRUST_SCORES}

class TrustUpdateRequest(BaseModel):
    model_id: str
    new_score: float = Field(..., ge=0.0, le=1.0)
    context: Optional[Dict[str, Any]] = None

@app.post("/arbiter/update-trust")
def update_trust_score(request: TrustUpdateRequest):
    # TODO: Replace with ML-driven update logic and Redis/Postgres persistence
    TRUST_SCORES[request.model_id] = request.new_score
    return {"success": True, "model_id": request.model_id, "new_score": request.new_score}

# TODO: Add endpoint to update trust/confidence scores from feedback event bus
# TODO: Add advanced arbitration policies, audit logging, and integration with admin dashboard 

if __name__ == "__main__":
    import uvicorn
    print("ðŸš€ Starting Value Function Arbiter on port 5016...")
    uvicorn.run(app, host="0.0.0.0", port=5016) 